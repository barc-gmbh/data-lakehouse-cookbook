{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# BARC Lakehouse Cookbook\n",
    " \n",
    "#### A stepwise introduction to modern data lakehouse technologies for business users\n",
    "\n",
    "*written by Thomas Zeutschler, Analyst at [BARC](https://barc.com) (Würzburg, Germany), sponsored by [Dremio](https://www.dremio.com)(Santa Clara, California, USA), provider of a next generation Data and Data Lakehouse platform*\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "## Step 2 - Moving from CSV to Parquet files\n",
    "Think of Parquet files as CSV files on steroids. They do anything better than CSV files, they are more compact, much faster, more efficient, and more secure. The only downside is you can't open them in a text editor. But who cares? We have Python.  \n",
    "\n",
    "### 2.1. Converting a CSV file to a Parquet file\n",
    "First, we need to convert the CSV file to a Parquet file. This can take some time, but it's worth to convert all your CSV files to Parquet, the reasons to do so, you will see in a few seconds. Here's how to convert a CSV file to a Parquet file using Python and Pandas."
   ],
   "id": "31abcd1937eb43bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T09:01:13.308784Z",
     "start_time": "2024-11-09T09:01:11.897670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('car_sales.csv')   # Load the CSV file\n",
    "df.to_parquet('car_sales.parquet')  # Save the DataFrame in the Parquet file format."
   ],
   "id": "4b92366290b47e44",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2. Now that we have the Parquet file, we can load and work with it.\n",
    "The approach for loading and analysing a Parquet file is the same as with the CSV file. The only difference is the file format. Let's load the Parquet file and find out **how many black BMWs were sold in 2015**."
   ],
   "id": "db9edc518abc33be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T09:01:13.618087Z",
     "start_time": "2024-11-09T09:01:13.326658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet('car_sales.parquet') # Load the Parquet file\n",
    "black_bmw = df.query('make == \"BMW\" and color == \"black\" and year == 2015')['vin'].count()\n",
    "print(f\"Number of black BMW sold in 2015: {black_bmw}, counted by VIN (vehicle identification number).\")"
   ],
   "id": "326207147c8686d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of black BMW sold in 2015: 168, counted by VIN (vehicle identification number).\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3. What's different now?\n",
    "**Nothing and everything**. As the approach is identical, we now have a file that is ±5x times smaller than the original CSV file. Think of 5x times less the storage cost, and think of Tera-Bytes and Peta-Bytes. \n",
    "\n",
    "Much more importantly, the loading and execution time was already much faster. You have processed and analyzed 1/2 a million records in a fraction of a second. Roughly 5x or more times faster using the CSV file. And please compare that with your Excel based workflow if this is where you're coming from. \n",
    "\n",
    "### 2.4 Final thoughts and takeaways   \n",
    "\n",
    "But **the real deal** is that you now work on exactly the same data format, that is used by almost all modern and powerful data and analytics technologies and platform out there, the [Parquet data format](https://parquet.apache.org).  \n",
    "\n",
    "*Please continue with the next step, where we will show you how to query CSV and Parquet files using DuckDB and SQL...*"
   ],
   "id": "d76df080f8f0b2e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bb1e9ea354e2edc4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
