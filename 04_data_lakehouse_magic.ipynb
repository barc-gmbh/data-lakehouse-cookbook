{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BARC Lakehouse Cookbook - Hands-On Tutorial\n",
    " \n",
    "#### A stepwise introduction to modern data lakehouse technologies for business users.\n",
    "\n",
    "*written by Thomas Zeutschler, Analyst at [BARC](https://barc.com), Würzburg, Germany*\n",
    " \n",
    "*sponsored by [Dremio](https://www.dremio.com), Santa Clara, California, USA, provider of a very modern Data Lakehouse platform*\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Working with CSV files](01_working_with_csv_files.ipynb)\n",
    "2. [Working with Parquet files](02_from_csv_to_parquet_files.ipynb)\n",
    "3. [Working with Delta Lake files](03_lightning_fast_sql_analysis.ipynb)\n",
    "4. [Working with Dremio](04_data_lakehouse_magic.ipynb)\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "## Step 4 - Let the Data Lakehouse Magic begin!\n",
    "\n",
    "### Step 4.1 - Query multiple files at once, even if they are different\n",
    "Up to now, we processed a single file only. But in real world business, you likely have multiple files representing your dataset, e.g., yearly, monthly or even daily files you received or downloaded. A common challenge: The files may even look different over time, new columns were added, others were removed, or their position changed. A true Nightmare with Excel, but quite easy with modern data tools. \n",
    "\n",
    "I have prepared a folder named `car_sales` with 3 small but differently structured data files, you may take a look at them in the folder `car_sales` in the same directory as this file. Let's see how we can join and process them all together. You won't believe... again it's just a single line of code."
   ],
   "id": "ac023e2919ed41c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T15:18:40.893021Z",
     "start_time": "2024-11-09T15:18:40.860890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import duckdb \n",
    "\n",
    "result = duckdb.sql(\"SELECT * FROM read_csv('car_sales/*.csv', union_by_name=true);\")\n",
    "print(result) "
   ],
   "id": "bcb5bc8fde7aa53d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬───────────┬──────────┬────────────┬─────────────┬──────────────┬──────────────┬─────────┐\n",
      "│ year  │   make    │  model   │    trim    │    body     │ sellingprice │ transmission │   mmr   │\n",
      "│ int64 │  varchar  │ varchar  │  varchar   │   varchar   │    double    │   varchar    │ double  │\n",
      "├───────┼───────────┼──────────┼────────────┼─────────────┼──────────────┼──────────────┼─────────┤\n",
      "│  2014 │ Kia       │ Sorento  │ LX         │ SUV         │      21500.0 │ NULL         │    NULL │\n",
      "│  2014 │ BMW       │ 3 Series │ 328i SULEV │ Sedan       │      30000.0 │ NULL         │    NULL │\n",
      "│  2014 │ Chevrolet │ Camaro   │ LT         │ Convertible │      17500.0 │ NULL         │    NULL │\n",
      "│  2015 │ Nissan    │ Altima   │ NULL       │ NULL        │      10900.0 │ NULL         │    NULL │\n",
      "│  2015 │ Ford      │ Fusion   │ SE         │ Sedan       │      12000.0 │ automatic    │ 15350.0 │\n",
      "│  2016 │ Kia       │ Sorento  │ LX         │ SUV         │      21500.0 │ automatic    │ 20600.0 │\n",
      "│  2016 │ Chevrolet │ Cruze    │ 2LT        │ Sedan       │      10600.0 │ automatic    │ 13900.0 │\n",
      "└───────┴───────────┴──────────┴────────────┴─────────────┴──────────────┴──────────────┴─────────┘\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 4.2 - Introducing the Data Lakehouse Concept\n",
    "What you received from the last query is a combined view of all your data from all the files in the `car_sales` folder. This approach is essentially what true Data Lakehouse formats like [Apache Iceberg](https://iceberg.apache.org) or [Delta](https://delta.io/) are designed to do. These formats allow you to query all your data at once, regardless of how many files you have, how large they are, or how their structure has evolved over time.\n",
    "\n",
    "However, the Data Lakehouse approach, exemplified by Apache Iceberg, goes much further by providing valuable features that make it not just an alternative to traditional data warehouse solutions, but often a superior choice for many use cases — faster, more cost-effective, and more enjoyable to work with. Here are just a few Data Lakehouse key features:\n",
    "\n",
    "- **ACID Transactions**: A crucial feature of any reliable database system, ensuring your data remains consistent and dependable, even in case of failures. With just files on your hard drive, as we have so far, this level of reliability can't be guaranteed.\n",
    "\n",
    "- **Schema Evolution**: You can add, remove, or modify columns in your data files without breaking existing queries. Attempting this with a traditional data warehouse is often a nightmare, requiring days, weeks, or even months of work.\n",
    "\n",
    "- **Time Travel**: Query your data as it was at any point in time, even if it has been altered or deleted. This enables like-for-like comparisons, such as assessing sales figures under old versus new sales hierarchies. Such comparisons are rarely feasible in traditional data warehouses without complex data modeling and additional tools.\n",
    "\n",
    "- **Incremental Data Processing**: Process only the new data added since the last update. This is essential for many business scenarios, such as updating sales figures on a monthly, weekly, daily, or even hourly basis. For extreme cases, you can even stream data into your data lakehouse and process it with just a 2-3 second latency for near real-time user interaction.\n",
    "\n",
    "### Step 4.3 - Death of Vendor Lock-In\n",
    "The Data Lakehouse concept is just that — a concept. To interact with your data, you need a query engine. The query engine reads and ideally also writes your data files, processes SQL queries, and returns results. It is the core of your Data Lakehouse. The great news? You can swap out the query engine anytime or even use multiple different engines concurrently without altering your data or queries. With a Data Lakehouse, you have the freedom to choose the best query engine for your specific needs, budget, or personal preferences. **No more vendor lock-in—no reliance on or commitment to a single vendor.** Traditional vendors hate this openness, and try everything to lock you in with proprietary formats and tools, but you don't have to play along anymore.\n",
    "\n",
    "### Step 4.4 - Performance Matters\n",
    "As demonstrated, DuckDB is a fast query engine for your local machine. But imagine scaling up from 500,000 records in a 20MB Parquet file to 10 billion records in tens of thousands of Parquet files totaling 400TB. This size represents only a typical medium-scale Data Lakehouse, not even a large one.\n",
    "\n",
    "Would you believe there are query engines that can process such datasets with sub-second response times? The sponsor of this cookbook, [Dremio](https://www.dremio.com), is one of them — likely among the fastest, if not **the fastest**. When compared to traditional data warehouses and other modern lakehouse platforms such as [Databricks](https://www.databricks.com), [Snowflake](https://www.snowflake.com), [Google BigQuery](https://cloud.google.com/bigquery), [Amazon Redshift](https://aws.amazon.com/redshift), or [Microsoft Fabric](https://www.microsoft.com/en-us/microsoft-fabric), Dremio stands out for being faster and more cost-efficient. The math in modern data processing is simple: faster processing means lower costs, as it requires less hardware and compute time. Tools like Dremio are often used as a query layer on top of cloud data warehouses to enhance speed, reduce costs, and improve user experience, e.g., Databricks and Dremio are a perfect match, as Databricks is great for data processing and data science and Dremio is great for business user focussed data engineering, data mesh implementations, data products and fast BI.\n",
    "\n",
    "Perhaps even more importantly, faster processing means less waiting, increased enjoyment, better user satisfaction, and quicker decisions, as **you can interact with your data in real-time without waiting for minutes or hours for queries to complete**. This is what modern data tools are about and what you should expect from your data platform.\n",
    "\n",
    "### Step 4.5 - The End of This Data Lakehouse Cookbook\n",
    "Unfortunately, I can’t provide a 400TB dataset for you to experiment with, nor the infrastructure to handle it. However, you can collaborate with your IT department to try out Dremio (or another tool) either in the cloud for a quick start or on-premises with your own infrastructure. Be warned: for larger datasets (terabytes to petabytes), you’ll need at least a small compute cluster of 4-5 nodes and a fast storage backend to achieve sub-second response times. No one defies the laws of physics—not even Dremio. I personally would start with cloud, as it's fast, easy, and you can scale up and down as needed, or even through it away if you don't like it.\n",
    "\n",
    "The Dremio team is known for being supportive, even if you're not yet a customer. Besides the big names mentioned above, other data engines and platforms compete to be the fastest and most user-friendly, such as [Presto](https://prestodb.io) and [Starburst](https://www.starburst.io). Likely it’s worth exploring them as well.\n",
    "\n",
    "***I hope this guide has inspired you to dive deeper into the world of Data Lakehouse.***\n",
    "\n",
    "***Thank you for reading!***\n",
    "\n",
    "Thomas Zeutschler, Analyst at BARC, *Data Lakehouse enthusiast and (data) software developer*  \n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Working with CSV files](01_working_with_csv_files.ipynb)\n",
    "2. [Working with Parquet files](02_from_csv_to_parquet_files.ipynb)\n",
    "3. [Working with Delta Lake files](03_lightning_fast_sql_analysis.ipynb)\n",
    "4. [Working with Dremio](04_data_lakehouse_magic.ipynb)\n",
    "\n",
    "Back to the [BARC Lakehouse Cookbook Homepage](readme.md)"
   ],
   "id": "31abcd1937eb43bf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
